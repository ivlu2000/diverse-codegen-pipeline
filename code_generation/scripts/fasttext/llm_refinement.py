"""
SECURITY WARNING: This script executes code generated by an LLM.
The code has been secured with multiple layers of protection:
1. AST-based security analysis to detect dangerous operations
2. Restricted execution environment with only safe builtins
3. Timeout protection to prevent infinite loops
4. Explicit instructions to the LLM to avoid dangerous operations

DANGEROUS OPERATIONS THAT ARE BLOCKED:
- File operations (open, os.*, pathlib.*, etc.)
- System commands (subprocess.*, os.system, etc.)
- Network access (urllib.*, requests.*, socket.*, etc.)
- Process control (multiprocessing.*, threading.*, signal.*, etc.)
- Database access (sqlite3.*, pickle.*, marshal.*, etc.)
- Input/output (input, eval, exec, compile, etc.)
- System info access (platform.*, psutil.*, resource.*, etc.)

The script will reject any code that attempts these operations.
"""

import argparse
import os
import random
import re
import sys
import time
import subprocess
import json
import uuid

import dask.dataframe as dd
import pandas as pd


# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.insert(0, project_root)

from src.shared.llm import LLMClient  # noqa: E402

llm = LLMClient(
    base_url="http://localhost:8000/v1",
    api_key="no-key-needed",
    model="Qwen/Qwen2.5-Coder-7B-Instruct",
)


# First prompt: Generate instruction and reasoning
prompt_1 = """
You are given a noisy, unstructured, or informal document. The document may contain one or more segments that can be reformulated into valid coding problems.

Your task is to:

1. Carefully read the document.
2. Decide whether it contains any ideas that can be formulated into distinct and meaningful coding problems.

* If **no viable coding problem ideas** can be derived from the document, respond **only** with the following exact sentence (do not include anything else):
  `The document does not contain content that can be reformulated into a coding problem.`

* If the document **does** contain one or more viable ideas for coding problems, proceed to step 3.

3. For each idea that qualifies:

   * Generate a complete coding problem in the specified format below.
   * Choose whether a function or a class is more appropriate for the task.
   * Include the problem statement and reasoning.
   * **IMPORTANT**: You MUST include **both tags** (`<instruction>`, `<reasoning>`) for **each problem**, even if multiple problems are found. **No tags may be omitted.**
   * **IMPORTANT**: After each problem block, insert a line containing **only** `<END></END>` — this tag is mandatory for correct parsing.
   * **IMPORTANT**: Do not modify tag names, and do not add extra spaces or alter their placement. Follow the template **exactly**.

---

For each coding problem found, return the following structure:

<instruction>  
**Title**: [Concise, clear title]  

**Problem Description**:
\[Write a full problem description derived from the document.]

**Constraints**:

* \[Define limits on input size, types, values, etc.]

**Example 1**:
Input: ...
Output: ...
Explanation: ...

**Example 2**:
Input: ...
Output: ...
Explanation: ... </instruction>

<reasoning>  
[Describe the high-level approach to solving the problem. Do not include actual code here.]  
</reasoning>

**<END></END> ← This tag must be present after every problem block.**

---

Here is the document to process:

<document> {document} </document>
"""

# Second prompt: Generate 3 solution-test pairs
prompt_2 = """
Based on the following instruction and reasoning, generate 3 different Python solutions with their corresponding test cases.

**Instruction**: {instruction}

**Reasoning**: {reasoning}

You MUST generate exactly 3 different solution-test pairs using the EXACT format below. Do not modify the tag names or structure.

**REQUIRED FORMAT - COPY EXACTLY:**

<solution_1>
```python
def example_function_1():
    # Your first solution implementation here
    pass
```
</solution_1>

<test_1>
```python
def test_solution_1():
    # Your first test cases here
    assert example_function_1() == expected_result

test_solution_1()
```
</test_1>

<solution_2>
```python
def example_function_2():
    # Your second solution implementation here
    pass
```
</solution_2>

<test_2>
```python
def test_solution_2():
    # Your second test cases here
    assert example_function_2() == expected_result

test_solution_2()
```
</test_2>

<solution_3>
```python
def example_function_3():
    # Your third solution implementation here
    pass
```
</solution_3>

<test_3>
```python
def test_solution_3():
    # Your third test cases here
    assert example_function_3() == expected_result

test_solution_3()
```
</test_3>

**CRITICAL REQUIREMENTS:**
1. You MUST use the exact tags: <solution_1>, <test_1>, <solution_2>, <test_2>, <solution_3>, <test_3>
2. Do not change, modify, or omit any tags
3. Generate exactly 3 different approaches/solutions
4. Each solution must be complete and runnable Python code
5. Each test suite must be comprehensive and test the corresponding solution
6. Do not include any explanations outside the code blocks
7. Make sure all solutions are syntactically correct Python code
8. **MANDATORY**: ALL code must be wrapped in ```python blocks - never use plain text for code
9. **MANDATORY**: Every solution and test must start with ```python and end with ```
10. **MANDATORY**: Do not include any code outside of the ```python blocks
11. **MANDATORY**: Each test function must call itself at the end (e.g., test_solution_1())
12. **MANDATORY**: Do not include any text, explanations, or comments outside the tags
13. **MANDATORY**: Your response must start with <solution_1> and end with </test_3>

**SECURITY REQUIREMENTS:**
14. **CRITICAL**: Do NOT use any of these dangerous operations in your code:
    - File operations: open(), os.*, pathlib.*, glob.*, shutil.*
    - System commands: subprocess.*, os.system(), os.popen()
    - Network access: urllib.*, requests.*, socket.*, http.*, https.*
    - Process control: multiprocessing.*, threading.*, signal.*
    - Database access: sqlite3.*, pickle.*, marshal.*
    - Input/output: input(), eval(), exec(), compile()
    - System info: platform.*, psutil.*, resource.*
15. **ONLY USE**: Python built-in functions and libraries

**START YOUR RESPONSE WITH THE FIRST TAG <solution_1> AND END WITH </test_3>**

**EXAMPLE OF CORRECT FORMAT:**
<solution_1>
```python
def solve_problem():
    return "solution"
```
</solution_1>

<test_1>
```python
def test_solve_problem():
    assert solve_problem() == "solution"

test_solve_problem()
```
</test_1>

**IMPORTANT: If you cannot follow this exact format, your response will be rejected.**
**DO NOT ADD ANY TEXT BEFORE <solution_1> OR AFTER </test_3>**
"""


def extract_code_from_markdown(text):
    """
    Extract code from markdown blocks more efficiently.
    """
    # Try multiple patterns in order of preference
    patterns = [
        r'```python\s*(.*?)\s*```',  # Standard markdown with python
        r'```\s*(.*?)\s*```',        # Standard markdown without language
        r'(.*)',                     # Fallback: entire content
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
    
    return None

def run_batch_in_apptainer(code_test_pairs, apptainer_image='python311.sif', timeout=10):
    """
    Run a batch of (solution, test) code pairs in a single Apptainer container with subprocess isolation.
    Returns a list of dicts with stdout, stderr, and returncode for each pair.
    """
    batch_dir = f"/tmp/batch_{uuid.uuid4().hex}"
    os.makedirs(batch_dir, exist_ok=True)
    code_files = []
    for i, (solution, test) in enumerate(code_test_pairs):
        code_path = os.path.join(batch_dir, f"code_{i}.py")
        with open(code_path, "w") as f:
            f.write(solution + "\n\n" + test)
        code_files.append(code_path)
    # Write the batch runner script
    batch_runner_path = os.path.join(batch_dir, "batch_runner.py")
    with open(batch_runner_path, "w") as f:
        f.write('''import subprocess\nimport os\nimport json\nimport sys\n\nbatch_dir = os.path.dirname(os.path.abspath(__file__))\nresults = []\nfor filename in sorted(os.listdir(batch_dir)):\n    if filename.startswith("code_") and filename.endswith(".py") and filename != "batch_runner.py":\n        code_path = os.path.join(batch_dir, filename)\n        try:\n            result = subprocess.run([sys.executable, code_path], capture_output=True, text=True, timeout=''' + str(timeout) + ''')\n            results.append({\n                "file": filename,\n                "stdout": result.stdout,\n                "stderr": result.stderr,\n                "returncode": result.returncode\n            })\n        except subprocess.TimeoutExpired:\n            results.append({\n                "file": filename,\n                "stdout": "",\n                "stderr": "Timeout",\n                "returncode": -1\n            })\nwith open(os.path.join(batch_dir, "results.json"), "w") as f:\n    json.dump(results, f)\n''')
    # Run the batch in Apptainer
    subprocess.run([
        'apptainer', 'exec',
        '--containall', '--no-home', '--network=none',
        '--bind', f'{batch_dir}:{batch_dir}',
        apptainer_image,
        'python', os.path.join(batch_dir, 'batch_runner.py')
    ], check=True)
    # Read results
    results_path = os.path.join(batch_dir, "results.json")
    with open(results_path) as f:
        results = json.load(f)
    # Clean up
    for path in code_files + [batch_runner_path, results_path]:
        try:
            os.remove(path)
        except Exception:
            pass
    try:
        os.rmdir(batch_dir)
    except Exception:
        pass
    return results


def test_solution_pairs_batch(solution_test_pairs, apptainer_image='python311.sif', timeout=10):
    """
    Test a batch of solution-test pairs using batch Apptainer execution.
    Returns a list of (True/False, error_message or None) for each pair.
    """
    results = run_batch_in_apptainer(solution_test_pairs, apptainer_image, timeout)
    output = []
    for res in results:
        if res["returncode"] == 0:
            output.append((True, None))
        else:
            msg = f"Apptainer execution failed.\nStdout: {res['stdout']}\nStderr: {res['stderr']}"
            output.append((False, msg))
    return output


def process_solutions_batch(instruction_reasoning_pairs, batch_size=5):
    """
    Process a batch of instruction/reasoning pairs to generate and test solutions.
    Returns a list of (instruction, reasoning, solution, tests) tuples for valid solutions.
    """
    results = []
    
    # Process in batches
    for i in range(0, len(instruction_reasoning_pairs), batch_size):
        batch = instruction_reasoning_pairs[i:i + batch_size]
        print(f"Processing solution batch {i//batch_size + 1}/{(len(instruction_reasoning_pairs) + batch_size - 1)//batch_size}")
        
        # Generate prompts for this batch and track valid indices
        batch_prompts = []
        valid_indices = []
        
        for j, (instruction, reasoning) in enumerate(batch):
            # Skip if the instruction indicates no viable problem
            if "does not contain content that can be reformulated into a coding problem" in instruction:
                continue
            
            second_prompt = prompt_2.format(instruction=instruction, reasoning=reasoning)
            
            # Check if prompt is too long (rough estimate: 1 token ≈ 4 characters)
            if len(second_prompt) > 30000:  # Conservative limit to avoid token length issues
                print(f"Prompt too long ({len(second_prompt)} chars), truncating instruction and reasoning")
                # Truncate instruction and reasoning to reasonable lengths
                truncated_instruction = instruction[:10000] + "..." if len(instruction) > 10000 else instruction
                truncated_reasoning = reasoning[:2000] + "..." if len(reasoning) > 2000 else reasoning
                second_prompt = prompt_2.format(instruction=truncated_instruction, reasoning=truncated_reasoning)
            
            batch_prompts.append(second_prompt)
            valid_indices.append(j)
        

        if not batch_prompts:
            continue
        
        # Get responses from LLM in batch
        try:
            batch_responses = llm.batched_inference(
                batch_prompts,
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.",
                num_concurrent=len(batch_prompts)
            )
        except Exception as e:
            print(f"Error in batched inference: {e}")
            print("Skipping this batch due to inference error")
            continue
        
        # Debug: Print batch sizes to verify they match
        print(f"Batch prompts: {len(batch_prompts)}, Batch responses: {len(batch_responses)}")
        if len(batch_prompts) != len(batch_responses):
            print(f"WARNING: Mismatch between prompts ({len(batch_prompts)}) and responses ({len(batch_responses)})")
            # Truncate to the smaller size to avoid index errors
            min_size = min(len(batch_prompts), len(batch_responses))
            batch_prompts = batch_prompts[:min_size]
            batch_responses = batch_responses[:min_size]
            valid_indices = valid_indices[:min_size]
        
        # Skip if no valid responses
        if not batch_responses:
            print("No valid responses received, skipping batch")
            continue
        
        # Filter out None or empty responses
        valid_responses = []
        valid_indices_filtered = []
        for i, response in enumerate(batch_responses):
            if response and response.strip():
                valid_responses.append(response)
                valid_indices_filtered.append(valid_indices[i])
        
        if not valid_responses:
            print("All responses were empty or None, skipping batch")
            continue
        
        batch_responses = valid_responses
        valid_indices = valid_indices_filtered
        
        
        # Process each response in the batch
        for response_idx, ((instruction, reasoning), response) in enumerate(zip([batch[idx] for idx in valid_indices], batch_responses)):
            
            # Parse the response to extract the 3 solution-test pairs
            solutions = []
            tests = []
            
            for k in range(1, 4):
                solution_match = re.search(
                    rf'<solution_{k}>\s*```python\s*(.*?)\s*```\s*</solution_{k}>', 
                    response, 
                    re.DOTALL
                )
                test_match = re.search(
                    rf'<test_{k}>\s*```python\s*(.*?)\s*```\s*</test_{k}>', 
                    response, 
                    re.DOTALL
                )
                
                print(f"Looking for solution_{k}: {'FOUND' if solution_match else 'NOT FOUND'}")
                print(f"Looking for test_{k}: {'FOUND' if test_match else 'NOT FOUND'}")
                
                # If not found, try alternative patterns
                if not solution_match:
                    # Try without python tag
                    solution_match = re.search(
                        rf'<solution_{k}>\s*```\s*(.*?)\s*```\s*</solution_{k}>', 
                        response, 
                        re.DOTALL
                    )
                    if solution_match:
                        print(f"Found solution_{k} with alternative pattern")
                
                if not test_match:
                    # Try without python tag
                    test_match = re.search(
                        rf'<test_{k}>\s*```\s*(.*?)\s*```\s*</test_{k}>', 
                        response, 
                        re.DOTALL
                    )
                    if test_match:
                        print(f"Found test_{k} with alternative pattern")
                
                if solution_match and test_match:
                    solutions.append(solution_match.group(1).strip())
                    tests.append(test_match.group(1).strip())
                    print(f"Successfully extracted solution_{k} and test_{k}")
                else:
                    print(f"Failed to extract solution_{k} or test_{k}")
                    # Debug: Show what we're looking for
                    print(f"Response: {response}")
                    print(f"Looking for pattern: <solution_{k}>...```python...```...</solution_{k}>")
                    print(f"Looking for pattern: <test_{k}>...```python...```...</test_{k}>")
            
            print(f"Total solutions found: {len(solutions)}")
            print(f"Total tests found: {len(tests)}")
            
            # Test each solution-test pair
            valid_pairs = []
            if solutions and tests:  # Only test if we have solutions and tests
                # Batch test all pairs at once
                batch_results = test_solution_pairs_batch(list(zip(solutions, tests)))
                for (solution, test), (valid, error_msg) in zip(zip(solutions, tests), batch_results):
                    if valid:
                        valid_pairs.append((solution, test))
                        print("Solution passed all tests!")
                    else:
                        print(f"Solution failed tests. Error: {error_msg}")
            else:
                print(f"No solutions or tests found for instruction {i+valid_indices[response_idx]+1}")
            
            # Select a random valid pair if any exist
            if valid_pairs:
                selected_solution, selected_tests = random.choice(valid_pairs)
                print(f"Selected solution from {len(valid_pairs)} valid options for instruction {i+valid_indices[response_idx]+1}.")
                results.append((instruction, reasoning, selected_solution, selected_tests))
            else:
                print(f"No valid solutions found for instruction {i+valid_indices[response_idx]+1}.")
    
    return results


def main():
    # parse the command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--offset", type=int, default=0)
    parser.add_argument("--take", type=int, default=100)
    args = parser.parse_args()

    offset = args.offset
    take = args.take

    root_dir = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    )

    parquet_path = os.path.join(
        root_dir, "datasets/fasttext/dclm/predictions/predictions.parquet"
    )
    os.makedirs(os.path.dirname(parquet_path), exist_ok=True)

    # Load or create the dataset with only the needed "text" column
    start = time.time()
    if not os.path.exists(parquet_path):
        print("Loading dataset from Hugging Face")
        df = dd.read_parquet(
            "hf://datasets/ivlu2000/dclm-baseline-fasttext/**/*.parquet",
            dtype_backend="pyarrow",
        )
        
        df.to_parquet(parquet_path, engine="pyarrow", compression="snappy")
        print(f"Dataset saved in {time.time() - start:.2f}s")
    else:
        print("Loading local Parquet dataset")
        df = dd.read_parquet(parquet_path, engine="pyarrow", columns=["text"])
        print(f"Dataset loaded in {time.time() - start:.2f}s")

    number_of_partitions = df.npartitions
    step_size = 1
    print("Offset", offset)
    print("Take", take)
    print("Number of partitions", number_of_partitions)

    dfs = [
        df.partitions[i : i + step_size]
        for i in range(
            offset,
            offset + take
            if offset + take < number_of_partitions
            else number_of_partitions,
            step_size,
        )
    ]

    final_df = pd.DataFrame(
        {
            "instruction": [],
            "reasoning": [],
            "solution": [],
            "tests": [],
        }
    )

    for df in dfs:
        df = df.compute()


        print("length of df", len(df))

        start = time.time()

        # Process in smaller chunks for better memory management
        chunk_size = len(df)  # Adjust based on your available memory
        all_instructions = []
        all_reasonings = []

        # First pass: Generate instructions and reasonings
        for i in range(0, len(df), chunk_size):
            chunk_df = df.iloc[i : i + chunk_size]
            prompts = [
                prompt_1.format(document=row["text"])
                for row in chunk_df.to_dict(orient="records")
            ]

            chunk_responses = llm.batched_inference(
                prompts,
                "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.",
                num_concurrent=len(prompts),
            )

            # Parse responses to extract instructions and reasonings
            for response in chunk_responses:
                results = response.split("<END></END>")
                for result in results:
                    instruction = re.search(
                        r"<instruction>(.*?)</instruction>", result, re.DOTALL
                    )
                    reasoning = re.search(
                        r"<reasoning>(.*?)</reasoning>", result, re.DOTALL
                    )
                    
                    if instruction and reasoning:
                        all_instructions.append(instruction.group(1).replace("\n", "", 1))
                        all_reasonings.append(reasoning.group(1).replace("\n", "", 1))

            # Print progress
            print(f"Processed {min(i + chunk_size, len(df))}/{len(df)} items for instruction/reasoning")

        # Second pass: Generate and test solutions in batches
        instruction_reasoning_pairs = list(zip(all_instructions, all_reasonings))
        batch_results = process_solutions_batch(instruction_reasoning_pairs, batch_size=len(df))


        
        # Extract results
        final_instructions = [item[0] for item in batch_results]
        final_reasonings = [item[1] for item in batch_results]
        final_solutions = [item[2] for item in batch_results]
        final_tests = [item[3] for item in batch_results]

        new_df = pd.DataFrame(
            {
                "instruction": final_instructions,
                "reasoning": final_reasonings,
                "solution": final_solutions,
                "tests": final_tests,
            }
        )


        print("length of new_df", len(new_df))
        print(f"Time taken: {time.time() - start:.2f}s")


        final_df = pd.concat([final_df, new_df])
        

    print("length of final_df", len(final_df))

    # create the directory if it doesn't exist
    os.makedirs(
        os.path.join(
            root_dir,
            f"datasets/fasttext/dclm/refined/refined_{offset}",
        ),
        exist_ok=True,
    )

    final_df.to_parquet(
        os.path.join(
            root_dir,
            f"datasets/fasttext/dclm/refined/refined_{offset}.parquet",
        ),
        engine="pyarrow",
        compression="snappy",
    )


if __name__ == "__main__":
    main()
